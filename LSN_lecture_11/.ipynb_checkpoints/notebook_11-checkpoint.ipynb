{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34930439-87cb-48f6-af27-ca37be46d586",
   "metadata": {},
   "source": [
    "**NOTEBOOK 11**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24590d96-e01f-4d10-8350-e29f2d0d5511",
   "metadata": {},
   "source": [
    "### Overview \n",
    "\n",
    "In this notebook our task will be to perform machine learning regression on noisy data with a Neural Network (NN).\n",
    "\n",
    "We will explore how the ability to fit depends on the structure of the NN. The goal is also to build intuition about why prediction is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d161904-349a-4f6d-aefc-5bae40b47745",
   "metadata": {},
   "source": [
    "### The Prediction Problem\n",
    "\n",
    "Consider a probabilistic process that gives rise to labeled data $(x,y)$. The data is generated by drawing samples from the equation\n",
    "\n",
    "$$\n",
    "    y_i= f(x_i) + \\eta_i,\n",
    "$$\n",
    "\n",
    "where $f(x_i)$ is some fixed, but (possibly unknown) function, and $\\eta_i$ is a Gaussian, uncorrelate noise variable such that\n",
    "\n",
    "$$\n",
    "\\langle \\eta_i \\rangle=0,   \\\\  \n",
    "\\langle \\eta_i \\eta_j \\rangle = \\delta_{ij} \\sigma\n",
    "$$\n",
    "\n",
    "We will refer to the $f(x_i)$ as the **true features** used to generate the data. \n",
    "\n",
    "To make predictions, we will consider a NN that depends on its parameters, weights and biases. The functions that the NN can model represent the **model class** that we are using to try to model the data and make predictions.\n",
    "\n",
    "To learn the parameters of the NN, we will train our models on a **training data set** and then test the effectiveness of the NN on a *different* dataset, the **validation data set**. The reason we must divide our data into a training and test dataset is that the point of machine learning is to make accurate predictions about new data we have not seen.\n",
    "\n",
    "To measure our ability to predict, we will learn our parameters by fitting our training dataset and then making predictions on our test data set. One common measure of predictive  performance of our algorithm is to compare the predictions,$\\{y_j^\\mathrm{pred}\\}$, to the true values $\\{y_j\\}$. A commonly employed measure for this is the sum of the mean square-error (MSE) on the test set:\n",
    "$$\n",
    "MSE= \\frac{1}{N_\\mathrm{test}}\\sum_{j=1}^{N_\\mathrm{test}} (y_j^\\mathrm{pred}-y_j)^2\n",
    "$$\n",
    "\n",
    "We will try to get a qualitative picture by examining plots on validation and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd5cdf-daa8-4b12-ae7c-b5370c94919f",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We start by considering the very simple case:\n",
    "$$\n",
    "f(x)=2x+1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec115b16-29bf-496d-b895-2c009d4a407f",
   "metadata": {},
   "source": [
    "**11.1 - LINEAR FIT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c6f02-f436-4515-bcd3-7536e918e7c6",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "In order to make practice with NN, explore how does the previous linear regression depend on the number of epochs, $N_{\\mathrm{epochs}}$, the number of data points $N_{\\mathrm{train}}$ and on the noise $\\sigma$. Try to improve the previous result operating on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd7525-c351-4752-91bd-4dfe62667104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams[\"figure.figsize\"]= (10,8)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b85a8e-9013-4786-8417-827ea94dc9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return (2*x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722af04-b245-4f7c-ac5f-07d727124f40",
   "metadata": {},
   "source": [
    "**MODEL 1 -- sigma = 0.4, epochs = 15, N_train = 1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce26df-a132-4a83-b0b1-126168cc5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "x_training = np.random.uniform(-1, 1, 1000)\n",
    "x_validation = np.random.uniform(-1, 1, 200)\n",
    "x_validation.sort()\n",
    "y_target = linear(x_validation)\n",
    "\n",
    "sigma = 0.4 # noise standard deviation\n",
    "y_training = np.random.normal(linear(x_training), sigma) #Generation of the measures\n",
    "y_validation = np.random.normal(linear(x_validation), sigma)\n",
    "\n",
    "# plot validation and target dataset\n",
    "plt.plot(x_validation, y_target, label='target')\n",
    "plt.scatter(x_validation, y_validation, color='g', label='validation data')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268f0da-8ae2-471e-8a9b-53674088ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the NN model exploiting TensorFlow and Keras\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse']) #FIX the optimizer, the loss function and the metrics\n",
    "\n",
    "# fit the model using training dataset\n",
    "# over 15 epochs of 32 batch size each\n",
    "history = model.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=15,\n",
    "          shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "          validation_data=(x_validation, y_validation))\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff7973-ff08-429f-9c1e-7e23b4674d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model \n",
    "score = model.evaluate(x_validation, y_validation, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551fcd8-dda1-4a08-a3dd-e4b4d1f5cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax0, ax1] = plt.subplots(1, 2, figsize=(30 , 12))\n",
    "\n",
    "ax0.plot(history.history['loss'])\n",
    "ax0.plot(history.history['val_loss'])\n",
    "ax0.set_ylabel('model loss')\n",
    "ax0.set_xlabel('epoch')\n",
    "ax0.legend(['train', 'test/validation'], loc='best')\n",
    "ax0.set_title('Loss -- Model 1')\n",
    "\n",
    "x_predicted = np.random.uniform(-1, 1, 200)\n",
    "y_predicted = model.predict(x_predicted)\n",
    "ax1.scatter(x_predicted, y_predicted,color='r', label=\"validation data\")\n",
    "ax1.plot(x_validation, y_target, label='target')\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.legend()\n",
    "ax1.set_title('Prediction -- Model 1')\n",
    "ax1.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aed086-9269-4020-8654-ad4b532ac6ae",
   "metadata": {},
   "source": [
    "**MODEL 2 -- sigma = 0.5, epochs = 30, N_train = 800**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a42ec6-e2f9-44c4-8cbe-e4f4ac33cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "x_training = np.random.uniform(-1, 1, 800)\n",
    "x_validation = np.random.uniform(-1, 1, 100)\n",
    "x_validation.sort()\n",
    "y_target = linear(x_validation)\n",
    "\n",
    "sigma = 0.5 # noise standard deviation\n",
    "y_training = np.random.normal(linear(x_training), sigma) #Generation of the measures\n",
    "y_validation = np.random.normal(linear(x_validation), sigma)\n",
    "\n",
    "# plot validation and target dataset\n",
    "plt.plot(x_validation, y_target, label='target')\n",
    "plt.scatter(x_validation, y_validation, color='g', label='validation data')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458ba90-4aa4-4693-9202-80589a84a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the NN model exploiting TensorFlow and Keras\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse']) #FIX the optimizer, the loss function and the metrics\n",
    "\n",
    "# fit the model using training dataset\n",
    "# over 15 epochs of 32 batch size each\n",
    "history = model.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "          validation_data=(x_validation, y_validation))\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592e39b-3b50-41ee-8c73-adff20c5221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model \n",
    "score = model.evaluate(x_validation, y_validation, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0663fd8-d255-4e01-8c6b-466924f7fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax0, ax1] = plt.subplots(1, 2, figsize=(30 , 12))\n",
    "\n",
    "ax0.plot(history.history['loss'])\n",
    "ax0.plot(history.history['val_loss'])\n",
    "ax0.set_ylabel('model loss')\n",
    "ax0.set_xlabel('epoch')\n",
    "ax0.legend(['train', 'test/validation'], loc='best')\n",
    "ax0.set_title('Loss -- Model 2')\n",
    "\n",
    "x_predicted = np.random.uniform(-1, 1, 100)\n",
    "y_predicted = model.predict(x_predicted)\n",
    "ax1.scatter(x_predicted, y_predicted,color='r', label=\"validation data\")\n",
    "ax1.plot(x_validation, y_target, label='target')\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.legend()\n",
    "ax1.set_title('Prediction -- Model 2')\n",
    "ax1.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c4c54-9472-4538-8743-255efdf38ffd",
   "metadata": {},
   "source": [
    "**MODEL 3 -- sigma = 0.1, epochs = 20, N_train = 400**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e727d-45c0-4ca6-91cc-ea3512e5145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "x_training = np.random.uniform(-1, 1, 400)\n",
    "x_validation = np.random.uniform(-1, 1, 50)\n",
    "x_validation.sort()\n",
    "y_target = linear(x_validation)\n",
    "\n",
    "sigma = 0.1 # noise standard deviation\n",
    "y_training = np.random.normal(linear(x_training), sigma) #Generation of the measures\n",
    "y_validation = np.random.normal(linear(x_validation), sigma)\n",
    "\n",
    "# plot validation and target dataset\n",
    "plt.plot(x_validation, y_target, label='target')\n",
    "plt.scatter(x_validation, y_validation, color='g', label='validation data')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201c9e6-c8a7-4942-aea0-f82100fef592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the NN model exploiting TensorFlow and Keras\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse']) #FIX the optimizer, the loss function and the metrics\n",
    "\n",
    "# fit the model using training dataset\n",
    "# over 15 epochs of 32 batch size each\n",
    "history = model.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=20,\n",
    "          shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "          validation_data=(x_validation, y_validation))\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa24456-668c-4657-b25e-cc9bfddd9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model \n",
    "score = model.evaluate(x_validation, y_validation, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246dec7e-fc8f-4421-b21a-175117398015",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax0, ax1] = plt.subplots(1, 2, figsize=(30 , 12))\n",
    "\n",
    "ax0.plot(history.history['loss'])\n",
    "ax0.plot(history.history['val_loss'])\n",
    "ax0.set_ylabel('model loss')\n",
    "ax0.set_xlabel('epoch')\n",
    "ax0.legend(['train', 'test/validation'], loc='best')\n",
    "ax0.set_title('Loss -- Model 3')\n",
    "\n",
    "x_predicted = np.random.uniform(-1, 1, 50)\n",
    "y_predicted = model.predict(x_predicted)\n",
    "ax1.scatter(x_predicted, y_predicted,color='r', label=\"validation data\")\n",
    "ax1.plot(x_validation, y_target, label='target')\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.legend()\n",
    "ax1.set_title('Prediction -- Model 3')\n",
    "ax1.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd39fe-c8e8-41e1-bdf7-c9443cc714fc",
   "metadata": {},
   "source": [
    "For what concerns model 1 and 3, the plots point out that they are not able to provide a good fit of the data, even though the scenario improves in model 1. This may be due to the fact that in model 3 there was low variability ($\\sigma$) and a small number of epochs and training data. In model 1 the variability and the dataset were augmented but the number of epochs remained the same, this resulted in a slight improvement of the prediction capability but not that satisfying. Model 2 seems to provide the best performance on the data, as in this case the variability was kept higher than in the other models and both the number of epochs and the size of the training data was kept sufficiently high but not with the risk to end up in overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa49e6-9bc9-4ac0-a1c0-7df66bfc5112",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Try to extend the model to obtain a reasonable fit of the following polynomial of order 3:\n",
    "\n",
    "$$\n",
    "f(x)=4-3x-2x^2+3x^3\n",
    "$$\n",
    "for $x \\in [-1,1]$.\n",
    "\n",
    "In order to make practice with NN, explore reasonable different choices for:\n",
    "\n",
    "- the number of layers\n",
    "- the number of neurons in each layer\n",
    "- the activation function\n",
    "- the optimizer\n",
    "- the loss function\n",
    "  \n",
    "Make graphs comparing fits for different NNs.\n",
    "Check your NN models by seeing how well your fits predict newly generated test data (including on data outside the range you fit. How well do your NN do on points in the range of $x$ where you trained the model? How about points outside the original training data set? \n",
    "Summarize what you have learned about the relationship between model complexity (number of parameters), goodness of fit on training data, and the ability to predict well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e44e8-fd87-4dfe-88ee-22ef3ea09101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polinomial(x):\n",
    "    return (4-3*x-2*(x**2)+3*(x**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c602f7-249e-4791-9d45-5c6fb9e83f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "x_training = np.random.uniform(-1, 1, 3000)\n",
    "x_validation = np.random.uniform(-1, 1, 100)\n",
    "x_validation.sort()\n",
    "y_target = polinomial(x_validation) # ideal (target) function\n",
    "\n",
    "sigma = 0.5 # noise standard deviation\n",
    "y_training = np.random.normal(polinomial(x_training), sigma) # actual measures from which we want to guess regression parameters\n",
    "y_validation = np.random.normal(polinomial(x_validation), sigma)\n",
    "\n",
    "# plot validation and target dataset\n",
    "plt.plot(x_validation, y_target, label='target')\n",
    "plt.scatter(x_validation, y_validation, color='r', label='validation data')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b6df7-02fd-4c72-ab75-d5cdca46cb59",
   "metadata": {},
   "source": [
    "In the following section of the notebook different models are investigated in order to fit the polinomial function considered. In particular, different models have different number of layers and number of neurons per layer, different activation functions and different optimizers. In some models there is a combination of different activation functions and optimizer. In this way, one can explore the space of the parameters of the models and investigate how the model behave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231a557-cecf-4770-8be1-4c5784b27daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - SELU Adam\n",
    "model_1 = tf.keras.Sequential()\n",
    "model_1.add(Dense(30, input_shape=(1,), activation='selu'))\n",
    "model_1.add(Dense(10, activation='selu'))\n",
    "model_1.add(Dense(1, activation='selu'))\n",
    "model_1.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Model 2 - ELU Nadam\n",
    "model_2 = tf.keras.Sequential()\n",
    "model_2.add(Dense(30, input_shape=(1,), activation='elu'))\n",
    "model_2.add(Dense(10, activation='elu'))\n",
    "model_2.add(Dense(1, activation='elu'))\n",
    "model_2.compile(optimizer='nadam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Model 3 - COMBINATION \n",
    "model_3 = tf.keras.Sequential()\n",
    "model_3.add(Dense(30, input_shape=(1,), activation='elu'))\n",
    "model_3.add(Dense(10, activation='selu'))\n",
    "model_3.add(Dense(1, activation='relu'))\n",
    "model_3.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Model 4 - COMBINATION\n",
    "model_4 = tf.keras.Sequential()\n",
    "model_4.add(Dense(30, input_shape=(1,), activation='elu'))\n",
    "model_4.add(Dense(10, activation='selu'))\n",
    "model_4.add(Dense(1, activation='relu'))\n",
    "model_4.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "\n",
    "# Model 5 -SIGMOID\n",
    "model_5 = tf.keras.Sequential()\n",
    "model_5.add(Dense(30, input_shape=(1,), activation='sigmoid'))\n",
    "model_5.add(Dense(10, activation='sigmoid'))\n",
    "model_5.add(Dense(1))\n",
    "model_5.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Model 6 - TANH\n",
    "model_6 = tf.keras.Sequential()\n",
    "model_6.add(Dense(30, input_shape=(1,), activation='tanh'))\n",
    "model_6.add(Dense(10, activation='tanh'))\n",
    "model_6.add(Dense(1))\n",
    "model_6.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Model 7 - COMBINATION pleteau and open bunds\n",
    "model_7 = tf.keras.Sequential()\n",
    "model_7.add(Dense(500, input_shape=(1,), activation='tanh'))\n",
    "model_7.add(Dense(30, activation='sigmoid'))\n",
    "model_7.add(Dense(10, activation='relu'))\n",
    "model_7.add(Dense(1, activation='selu'))\n",
    "model_7.compile(optimizer='adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b218c-d5ba-460a-8664-132581d74291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1\n",
    "history_1 = model_1.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_validation, y_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af430c1-b712-4ffe-bd79-44f5159128c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2\n",
    "history_2 = model_2.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_validation, y_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01436309-f078-4da7-9187-e061adc26d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 3\n",
    "history_3 = model_3.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_validation, y_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a486d8-e565-44a3-9b3e-e191c010cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 4\n",
    "history_4 = model_4.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_validation, y_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125895ff-85b5-46c8-bbe2-83d161948713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 5\n",
    "history_5 = model_5.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_validation, y_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3359e0f-7c00-4f17-b7e9-8576bc6e61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 6\n",
    "history_6 = model_6.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_validation, y_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b12e4d-36dc-47ef-8662-3dd4669db457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 7\n",
    "history_7 = model_7.fit(x=x_training, y=y_training, \n",
    "          batch_size=32, epochs=30,\n",
    "          shuffle=True, \n",
    "          validation_data=(x_validation, y_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77f593-0c8e-4a19-a215-9c1a5d484e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate Models\n",
    "score_1 = model_1.evaluate(x_validation, y_validation, batch_size=32, verbose=0)\n",
    "score_2 = model_2.evaluate(x_validation, y_validation, batch_size=32, verbose=0)\n",
    "score_3 = model_3.evaluate(x_validation, y_validation, batch_size=32, verbose=0)\n",
    "score_4 = model_4.evaluate(x_validation, y_validation, batch_size=32, verbose=0)\n",
    "score_5 = model_5.evaluate(x_validation, y_validation, batch_size=32, verbose=0)\n",
    "score_6 = model_6.evaluate(x_validation, y_validation, batch_size=32, verbose=0)\n",
    "score_7 = model_7.evaluate(x_validation, y_validation, batch_size=32, verbose=0)\n",
    "\n",
    "print('Model 1:')\n",
    "print('\\tTest loss:', score_1[0])\n",
    "print('\\tTest accuracy:', score_1[1])\n",
    "\n",
    "print('Model 2:')\n",
    "print('\\tTest loss:', score_2[0])\n",
    "print('\\tTest accuracy:', score_2[1])\n",
    "\n",
    "print('Model 3:')\n",
    "print('\\tTest loss:', score_3[0])\n",
    "print('\\tTest accuracy:', score_3[1])\n",
    "\n",
    "print('Model 4:')\n",
    "print('\\tTest loss:', score_4[0])\n",
    "print('\\tTest accuracy:', score_4[1])\n",
    "\n",
    "print('Model 5:')\n",
    "print('\\tTest loss:', score_5[0])\n",
    "print('\\tTest accuracy:', score_5[1])\n",
    "\n",
    "print('Model 6:')\n",
    "print('\\tTest loss:', score_6[0])\n",
    "print('\\tTest accuracy:', score_6[1])\n",
    "\n",
    "print('Model 7:')\n",
    "print('\\tTest loss:', score_7[0])\n",
    "print('\\tTest accuracy:', score_7[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6c17-6b71-4b2d-89da-e51999c6e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU Actitivation\n",
    "fig, [ax0, ax1, ax2] = plt.subplots(1, 3, figsize=(30 , 12))\n",
    "\n",
    "ax0.plot(history_1.history['loss'])\n",
    "ax0.plot(history_1.history['val_loss'])\n",
    "ax0.set_ylabel('model loss')\n",
    "ax0.set_xlabel('epoch')\n",
    "ax0.legend(['train', 'test/validation'], loc='best')\n",
    "ax0.set_title('Model 1 = loss')\n",
    "\n",
    "ax1.plot(history_2.history['loss'])\n",
    "ax1.plot(history_2.history['val_loss'])\n",
    "ax1.set_ylabel('model loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['train', 'test/validation'], loc='best')\n",
    "ax1.set_title('Model 2 = loss')\n",
    "\n",
    "ax2.plot(history_3.history['loss'])\n",
    "ax2.plot(history_3.history['val_loss'])\n",
    "ax2.set_ylabel('model loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['train', 'test/validation'], loc='best')\n",
    "ax2.set_title('Model 3 = loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91aa010-28eb-4d93-ae0b-316dc595ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination RELU, SELU, ELU\n",
    "fig, [ax0, ax1] = plt.subplots(1, 2, figsize=(30 , 12))\n",
    "\n",
    "ax0.plot(history_4.history['loss'])\n",
    "ax0.plot(history_4.history['val_loss'])\n",
    "ax0.set_ylabel('model loss')\n",
    "ax0.set_xlabel('epoch')\n",
    "ax0.legend(['train', 'test/validation'], loc='best')\n",
    "ax0.set_title('Model 4 = loss')\n",
    "\n",
    "ax1.plot(history_5.history['loss'])\n",
    "ax1.plot(history_5.history['val_loss'])\n",
    "ax1.set_ylabel('model loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['train', 'test/validation'], loc='best')\n",
    "ax1.set_title('Model 5 = loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd3a2e-fadd-4e14-af0b-4aad4c10991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function with plateau\n",
    "fig, [ax0, ax1] = plt.subplots(1, 2, figsize=(30 , 12))\n",
    "\n",
    "ax0.plot(history_6.history['loss'])\n",
    "ax0.plot(history_6.history['val_loss'])\n",
    "ax0.set_ylabel('model loss')\n",
    "ax0.set_xlabel('epoch')\n",
    "ax0.legend(['train', 'test/validation'], loc='best')\n",
    "ax0.set_title('Model 6 = loss')\n",
    "\n",
    "ax1.plot(history_7.history['loss'])\n",
    "ax1.plot(history_7.history['val_loss'])\n",
    "ax1.set_ylabel('model loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['train', 'test/validation'], loc='best')\n",
    "ax1.set_title('Model 7 = loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790dc3bb-330d-475c-8b10-f01878086d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation with no bounds\n",
    "x_predicted = np.random.uniform(-1, 1, 500)\n",
    "y_predicted_1 = model_1.predict(x_predicted)\n",
    "y_predicted_2 = model_2.predict(x_predicted)\n",
    "y_predicted_3 = model_3.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted_1,color='g', label=\"Model 1\")\n",
    "plt.scatter(x_predicted, y_predicted_2,color='r', label=\"Model 2\")\n",
    "plt.scatter(x_predicted, y_predicted_3,color='b', label=\"Model 3\")\n",
    "plt.plot(x_validation, y_target, label='target')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title('Predicted data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f51fc-e4c5-415a-aa37-d2410733c350",
   "metadata": {},
   "source": [
    "#### OUT OF BORDERS MODEL 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1dc2f5-0d87-470f-9cf1-5ca14d16f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-2., 2., 0.1)\n",
    "x_predicted = np.random.uniform(-2., 2., 500)\n",
    "y_predicted_1 = model_1.predict(x_predicted)\n",
    "y_predicted_2 = model_2.predict(x_predicted)\n",
    "y_predicted_3 = model_3.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted_1,color='g', label=\"Model 1\")\n",
    "plt.scatter(x_predicted, y_predicted_2,color='r', label=\"Model 2\")\n",
    "plt.scatter(x_predicted, y_predicted_3,color='b', label=\"Model 3\")\n",
    "plt.plot(x, polinomial(x), label='target')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title('Predicted data - no bounds activation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f090da-4b14-4a18-abb9-5deee5df0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination RELU, ELU, SELU Actitivation\n",
    "x_predicted = np.random.uniform(-1, 1, 500)\n",
    "y_predicted_4 = model_4.predict(x_predicted)\n",
    "y_predicted_5 = model_5.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted_4,color='r', label=\"Model 4\")\n",
    "plt.scatter(x_predicted, y_predicted_5,color='g', label=\"Model 5\")\n",
    "plt.plot(x_validation, y_target, label='target')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title('Predicted data - Combination')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00907bf-0397-410d-8cca-e21b08b5e772",
   "metadata": {},
   "source": [
    "#### OUT OF BORDERS MODEL 4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164815f-15ab-4665-805d-e5ca73b5e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination RELU, ELU, SELU Actitivation\n",
    "x = np.arange(-2., 2., 0.1)\n",
    "x_predicted = np.random.uniform(-2., 2., 500)\n",
    "y_predicted_4 = model_4.predict(x_predicted)\n",
    "y_predicted_5 = model_5.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted_4,color='r', label=\"Model 4\")\n",
    "plt.scatter(x_predicted, y_predicted_5,color='g', label=\"Model 5\")\n",
    "plt.plot(x, polinomial(x), label='target')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title('Predicted data - Combination')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335fa0d-4252-4635-913a-23d2ef4a5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function with plateau\n",
    "x_predicted = np.random.uniform(-1, 1, 500)\n",
    "y_predicted_6 = model_6.predict(x_predicted)\n",
    "y_predicted_7 = model_7.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted_6,color='r', label=\"Model 6\")\n",
    "plt.scatter(x_predicted, y_predicted_7,color='g', label=\"Model 7\")\n",
    "plt.plot(x_validation, y_target, label='target')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title('Predicted data - plateu activation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6035e-56f2-456f-8c4a-ba9235076fd9",
   "metadata": {},
   "source": [
    "#### OUT OF BORDERS MODEL 6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a6f92-2296-413e-ac18-cb0638bfc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function with plateau\n",
    "x = np.arange(-2., 2., 0.1)\n",
    "x_predicted = np.random.uniform(-2., 2., 500)\n",
    "y_predicted_6 = model_6.predict(x_predicted)\n",
    "y_predicted_7 = model_7.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted_6,color='r', label=\"Model 6\")\n",
    "plt.scatter(x_predicted, y_predicted_7,color='g', label=\"Model 7\")\n",
    "plt.plot(x, polinomial(x), label='target')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title('Predicted data - plateu activation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d452164-0652-439c-824b-c712ed0fff96",
   "metadata": {},
   "source": [
    "Models 1,2,3 seems to have a good prediction capability except for the tail, in which all of them fails evidently (with particular regard for model 3 that presents a linear trend). For the combination graph (model 4) the same considerations hold. For models 5,6,7, in which the activation functions are plateu-type (sigmoid and tanh) and a combination of these and RELU/SELU, the predictions agree with the target function also in the tail for model 6 and 7, while in model 5 they fails completely. Therefore one must say that the best performance is given by model 7 and it must be noted that in this NN the first layer is composed of 500 neurons with tanh activation function: this gives depth to the NN and therefore the model performs better than the others.  \n",
    "For all the models investigated, the predictions outside the interval considered $[-1, 1]$ fail (the interval was extended to $[-2, 2])$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8123c0d-e271-493b-a114-0900d4fa8146",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "  \n",
    "Try to extend the model to fit a simple trigonometric 2D function such as $f(x,y) = \\sin(x^2+y^2)$ in the range $x \\in [-3/2,3/2]$ and $y \\in [-3/2,3/2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b931829-1df1-48e2-8321-c2570f0de68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x,y):\n",
    "    return np.sin(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8222e06-4577-4fc7-a907-7292f3149116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "data_training = np.random.uniform(-1.5, 1.5, size=(10000, 2))\n",
    "data_validation = np.random.uniform(-1.5, 1.5, size=(1000, 2))\n",
    "data_validation.sort()\n",
    "target = sin(data_validation[0], data_validation[1])\n",
    "\n",
    "sigma = 0.5 # noise standard deviation\n",
    "f_training = np.random.normal(sin(data_training[:,0], data_training[:,1]), sigma)\n",
    "f_validation = np.random.normal(sin(data_validation[:,0], data_validation[:,1]), sigma)\n",
    "\n",
    "test_points = np.random.uniform(-1.5, 1.5, size=(2, 50))\n",
    "test_points.sort()\n",
    "x, y = np.meshgrid(test_points[0], test_points[1])\n",
    "z_target= sin(x,y)\n",
    "z_validation = np.random.normal(sin(x, y), sigma)\n",
    "\n",
    "fig = plt.figure(figsize=(18,13))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x, y, z_validation, color='green', linewidth=0.1)\n",
    "ax.contour3D(x, y, z_target, 30, cmap='plasma')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Training data', fontsize=20)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5dd8f3-bfbe-4106-8914-a3caddb6a93d",
   "metadata": {},
   "source": [
    "Given the complexity of the function, a NN with 4 layers of 30, 20, 10 and a 1 neurons with RELU activation function was designed to address the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de423a-4129-46d3-a6a7-17a912fbedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(30, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed02a93-2bea-4a1d-b34b-470f5f054ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Model 1 using training dataset\n",
    "history = model.fit(x=data_training, y=f_training, \n",
    "          batch_size=32, epochs=50,\n",
    "          shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "          validation_data=(data_validation, f_validation),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a647b-c23f-4731-ad5f-82903c6d473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(data_validation, f_validation, batch_size=32, verbose=1)\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342ca54-bb57-4501-ae5f-8378ba7b9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test/validation'], loc='best')\n",
    "plt.title('Model - loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbf031-5e19-4c1b-aaf9-da11ea3f825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted = np.random.uniform(-1.5, 1.5,size=(1000,2))\n",
    "f_predicted = model.predict(data_predicted)\n",
    "\n",
    "test_points = np.random.uniform(-1.5, 1.5, size=(2, 50))\n",
    "test_points.sort()\n",
    "x, y = np.meshgrid(test_points[0], test_points[1])\n",
    "z_target= sin(x,y)\n",
    "\n",
    "fig = plt.figure(figsize=(18,13))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(data_predicted[:,0], data_predicted[:,1], f_predicted, color='green', linewidth=0.1, label='predicted')\n",
    "ax.contour3D(x, y, z_target, 30, cmap='plasma')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.legend()\n",
    "ax.set_title('Predicted data', fontsize=20)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11376f-97b0-4fb9-b459-0af7f7f94ba9",
   "metadata": {},
   "source": [
    "The 3D plot shows a good agreement between the prediction and the target function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
